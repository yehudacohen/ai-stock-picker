{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669a368-458b-45a1-9241-71b84ec3ff42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git peft wandb bitsandbytes datasets python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c047fc-e82c-45ff-9648-5da4340cdc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getcwd\n",
    "\n",
    "load_dotenv(f'{getcwd()}/env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5950d-4e06-4935-9d44-21ea76ff1eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    TrainerCallback,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    ModernBertForSequenceClassification\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ea091-1ed6-4b2e-b7c4-3214d6b40fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    base_model: str = \"answerdotai/modernbert-large\"\n",
    "    max_length: int = 3300\n",
    "    batch_size: int = 3  # Optimized for L4 GPU\n",
    "    num_labels: int = 6\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "    num_epochs: int = 3\n",
    "    warmup_ratio: float = 0.1\n",
    "    gradient_accumulation_steps: int = 2\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup GPU environment and optimize settings\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"This script requires GPU acceleration\")\n",
    "    \n",
    "    device = torch.cuda.current_device()\n",
    "    gpu_properties = torch.cuda.get_device_properties(device)\n",
    "    vram_gb = gpu_properties.total_memory / 1024**3\n",
    "    logger.info(f\"GPU detected: {gpu_properties.name} with {vram_gb:.2f}GB VRAM\")\n",
    "    \n",
    "    # Optimize settings for L4 GPU\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    return device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec89d07-fa29-47cb-9f47-7da38cf499b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for dynamic padding\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        raise ValueError(\"Empty batch after filtering\")\n",
    "\n",
    "    # Ensure all batch items have the same keys and structure\n",
    "    input_ids = pad_sequence([b['input_ids'] for b in batch], batch_first=True)\n",
    "    attention_mask = pad_sequence([b['attention_mask'] for b in batch], batch_first=True)\n",
    "    labels = torch.stack([b['labels'] for b in batch])\n",
    "    metadata = [b['metadata'] for b in batch]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15aef25-22cc-4ac6-be05-5f272804f782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicTextDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 3500):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self._length = len(dataframe)\n",
    "\n",
    "    @property\n",
    "    def column_names(self):\n",
    "        return ['input_ids', 'attention_mask', 'labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):  # Handle batched indexing\n",
    "            batch = [self._get_single_item(i) for i in idx]\n",
    "            return self._collate_batch(batch)\n",
    "        elif isinstance(idx, slice):  # Handle slice indexing\n",
    "            start, stop, step = idx.indices(len(self))\n",
    "            batch = [self._get_single_item(i) for i in range(start, stop, step)]\n",
    "            return self._collate_batch(batch)\n",
    "        elif isinstance(idx, int):\n",
    "            return self._get_single_item(idx) #single item dictionary\n",
    "        else:\n",
    "            raise TypeError(f\"Invalid index type: {type(idx)}. Expected int, list, or slice.\")\n",
    "\n",
    "    def _get_single_item(self, idx):\n",
    "        if idx < 0:\n",
    "            idx += len(self)\n",
    "        if idx >= self._length or idx < 0:\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of size {self._length}\")\n",
    "\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = self._read_file(row['snapshot_file'])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.float)\n",
    "\n",
    "            \n",
    "        return {\n",
    "              'input_ids': encoding['input_ids'],\n",
    "              'attention_mask': encoding['attention_mask'],\n",
    "              'labels': labels,\n",
    "              'metadata': {\n",
    "                'file_path': row['snapshot_file'],\n",
    "                'index': idx,\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def _collate_batch(self, batch):\n",
    "        \"\"\"Collate a batch of individual samples into a single batch dictionary\"\"\"\n",
    "        collated = {key: [] for key in batch[0]}\n",
    "        for sample in batch:\n",
    "            for key, value in sample.items():\n",
    "                collated[key].append(value)\n",
    "\n",
    "        # Convert lists to tensors where applicable\n",
    "        collated['input_ids'] = torch.tensor(collated['input_ids'], dtype=torch.long)\n",
    "        collated['attention_mask'] = torch.tensor(collated['attention_mask'], dtype=torch.long)\n",
    "        collated['labels'] = torch.stack(collated['labels'])\n",
    "\n",
    "        return collated\n",
    "\n",
    "    def _read_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "            return \"[ERROR] File could not be read\"\n",
    "\n",
    "    def _parse_labels(self, labels):\n",
    "        try:\n",
    "            return torch.tensor(labels, dtype=torch.float)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing labels: {labels}. Exception: {e}\")\n",
    "            return torch.tensor(0.0, dtype=torch.float)\n",
    "\n",
    "    # Add this method for HuggingFace Trainer compatibility\n",
    "    def remove_columns(self, column_names):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f234072-f473-4dae-aa13-c4e3433d917b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def split_data_by_labels(df):\n",
    "    \"\"\"Split dataframe based on NaN labels:\n",
    "    - First df: rows with no NaN labels\n",
    "    - Second df: rows with some NaN labels \n",
    "    - Third df: rows with all NaN labels\n",
    "    \"\"\"\n",
    "    # Check if labels are all NaN, no NaNs, or some NaNs\n",
    "    df_train_val = df[~df['labels'].apply(lambda x: pd.isna(x).any())]  # No NaN values at all\n",
    "    all_nans = df[df['labels'].apply(lambda x: pd.isna(x).all())]   # All values are NaN\n",
    "    some_nans = df[df['labels'].apply(lambda x: pd.isna(x).any() & ~pd.isna(x).all())]  # Mix of NaN and non-NaN\n",
    "    \n",
    "    return df_train_val, some_nans, all_nans\n",
    "\n",
    "def rolling_window_split(\n",
    "    df: pd.DataFrame,\n",
    "    train_window_years: int = 5,\n",
    "    validation_years: int = 1,\n",
    "    min_train_years: int = 3,\n",
    "    stride: int = 1\n",
    ") -> List[Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Create rolling window splits based on years with proper temporal separation.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with 'year' column\n",
    "        train_window_years: Number of years to include in training window\n",
    "        validation_years: Number of years to use for validation\n",
    "        min_train_years: Minimum number of years required for training\n",
    "        stride: Number of years to move forward in each split\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_df, val_df) tuples\n",
    "    \"\"\"\n",
    "    years = sorted(df['year'].unique())\n",
    "    splits = []\n",
    "    \n",
    "    # Calculate the total window size\n",
    "    total_window = train_window_years + validation_years\n",
    "    \n",
    "    # Generate splits\n",
    "    for start_idx in range(0, len(years) - total_window + 1, stride):\n",
    "        # Define the windows\n",
    "        train_start = years[start_idx]\n",
    "        train_end = years[start_idx + train_window_years - 1]\n",
    "        val_start = years[start_idx + train_window_years]\n",
    "        val_end = years[start_idx + total_window - 1]\n",
    "        \n",
    "        # Create the splits\n",
    "        train_df = df[\n",
    "            (df['year'] >= train_start) & \n",
    "            (df['year'] <= train_end)\n",
    "        ]\n",
    "        \n",
    "        val_df = df[\n",
    "            (df['year'] >= val_start) & \n",
    "            (df['year'] <= val_end)\n",
    "        ]\n",
    "        \n",
    "        # Only add if we have enough training data\n",
    "        if len(train_df['year'].unique()) >= min_train_years:\n",
    "            splits.append((train_df, val_df))\n",
    "            \n",
    "            # Log the split information\n",
    "            logger.info(f\"\"\"\n",
    "            Created split:\n",
    "            Training: {train_start}-{train_end} ({len(train_df)} samples)\n",
    "            Validation: {val_start}-{val_end} ({len(val_df)} samples)\n",
    "            \"\"\")\n",
    "    \n",
    "    if not splits:\n",
    "        logger.warning(\"No valid splits were created with the given parameters\")\n",
    "    else:\n",
    "        logger.info(f\"Created {len(splits)} total splits\")\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0805ae-e34e-47d7-8b99-74d4d58ef577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def create_model(config: ModelConfig):\n",
    "    \"\"\"Create and configure the model with QLoRA\"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch.float32,  # Use float32 for compute\n",
    "    #     bnb_4bit_use_double_quant=False  # Disable double quantization\n",
    "    # )\n",
    "\n",
    "    model = ModernBertForSequenceClassification.from_pretrained(\n",
    "        config.base_model,\n",
    "        num_labels=config.num_labels,\n",
    "        problem_type=\"regression\",\n",
    "        torch_dtype=torch.bfloat16        \n",
    "    )\n",
    "    \n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"Wqkv\", \"Wo\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    if np.isnan(predictions).any():\n",
    "        logger.error(f\"NaNs found in predictions! {predictions}\")\n",
    "        raise ValueError(\"NaNs in predictions\")\n",
    "\n",
    "    mse = mean_squared_error(labels, predictions, multioutput='raw_values')\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(predictions - labels), axis=0)\n",
    "    \n",
    "    metrics = {\n",
    "        f\"mse_label{i+1}\": m for i, m in enumerate(mse)\n",
    "    }\n",
    "    metrics.update({\n",
    "        f\"rmse_label{i+1}\": m for i, m in enumerate(rmse)\n",
    "    })\n",
    "    metrics.update({\n",
    "        f\"mae_label{i+1}\": m for i, m in enumerate(mae)\n",
    "    })\n",
    "    \n",
    "    metrics[\"avg_mse\"] = np.mean(mse)\n",
    "    metrics[\"avg_rmse\"] = np.mean(rmse)\n",
    "    metrics[\"avg_mae\"] = np.mean(mae)\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97477057-0a29-4a15-8668-9f83c8a13ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class RankCombinationMethod(Enum):\n",
    "    AVERAGE = \"average\"\n",
    "    MINIMUM = \"minimum\"\n",
    "    WEIGHTED_AVERAGE = \"weighted_average\"\n",
    "\n",
    "class FinancialRanking:\n",
    "    def __init__(self, weights: Optional[Dict[str, float]] = None):\n",
    "        \"\"\"\n",
    "        Initializes the FinancialRanking class.\n",
    "        \n",
    "        Args:\n",
    "            weights (Dict[str, float]): Weights for different prediction periods\n",
    "                Default: {\"next_quarter\": 0.2, \"next_six_months\": 0.3, \"next_year\": 0.5}\n",
    "        \"\"\"\n",
    "        self.weights = weights or {\"next_quarter\": 0.2, \"next_six_months\": 0.3, \"next_year\": 0.5}\n",
    "        if not np.isclose(sum(self.weights.values()), 1.0):\n",
    "            raise ValueError(\"Weights must sum to 1\")\n",
    "\n",
    "    def _combine_ranks(self, row: pd.Series, method: RankCombinationMethod) -> float:\n",
    "        \"\"\"\n",
    "        Combines per-period ranks into a single score.\n",
    "        \n",
    "        Args:\n",
    "            row (pd.Series): Row containing period ranks\n",
    "            method (RankCombinationMethod): Method to combine ranks\n",
    "            \n",
    "        Returns:\n",
    "            float: Combined rank score\n",
    "        \"\"\"\n",
    "        ranks = [\n",
    "            row['next_quarter_rank'],\n",
    "            row['next_six_months_rank'],\n",
    "            row['next_year_rank']\n",
    "        ]\n",
    "        \n",
    "        if method == RankCombinationMethod.AVERAGE:\n",
    "            return np.mean(ranks)\n",
    "        elif method == RankCombinationMethod.MINIMUM:\n",
    "            return np.min(ranks)\n",
    "        elif method == RankCombinationMethod.WEIGHTED_AVERAGE:\n",
    "            return (\n",
    "                ranks[0] * self.weights['next_quarter'] +\n",
    "                ranks[1] * self.weights['next_six_months'] +\n",
    "                ranks[2] * self.weights['next_year']\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported rank combination method: {method}\")\n",
    "\n",
    "    def rank_stocks(self, df: pd.DataFrame, combination_method: RankCombinationMethod = RankCombinationMethod.WEIGHTED_AVERAGE) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ranks stocks based on predictions using the specified combination method.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with predictions column containing arrays of predictions\n",
    "            combination_method (RankCombinationMethod): Method to combine period ranks\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with added ranking columns and sorted by final rank\n",
    "        \"\"\"\n",
    "        # Convert predictions to numpy arrays if they aren't already\n",
    "        df = df.copy()\n",
    "        df['predictions'] = df['predictions'].apply(lambda x: np.array(x) if not isinstance(x, np.ndarray) else x)\n",
    "        \n",
    "        # Create ranking dataframe for each prediction period\n",
    "        prediction_df = pd.DataFrame({\n",
    "            'ticker': df.index,\n",
    "            'next_quarter': df['predictions'].apply(lambda x: np.mean(x[:2])),\n",
    "            'next_six_months': df['predictions'].apply(lambda x: np.mean(x[2:4])),\n",
    "            'next_year': df['predictions'].apply(lambda x: np.mean(x[4:]))\n",
    "        })\n",
    "        \n",
    "        # Calculate ranks for each period (higher predictions get lower ranks)\n",
    "        prediction_df['next_quarter_rank'] = prediction_df['next_quarter'].rank(ascending=False)\n",
    "        prediction_df['next_six_months_rank'] = prediction_df['next_six_months'].rank(ascending=False)\n",
    "        prediction_df['next_year_rank'] = prediction_df['next_year'].rank(ascending=False)\n",
    "        \n",
    "        # Calculate combined rank\n",
    "        prediction_df['combined_score'] = prediction_df.apply(\n",
    "            lambda row: self._combine_ranks(row, combination_method), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Add ranks back to original dataframe\n",
    "        df_result = pd.concat([\n",
    "            df,\n",
    "            prediction_df[['next_quarter_rank', 'next_six_months_rank', 'next_year_rank', 'combined_score']]\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Sort by combined score and add final rank\n",
    "        df_result = df_result.sort_values('combined_score')\n",
    "        df_result['rank'] = range(1, len(df_result) + 1)\n",
    "        \n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86defa-6f5e-48c6-a477-2aa1529da59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "class ScaledLossTrainer(Trainer):\n",
    "    \n",
    "    def training_step(self, model, inputs, optimizer=None):\n",
    "        # print(f\"Labels: {inputs['labels']}, metadata: {inputs['metadata']}\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "#         # check if loss is na and if so print input labels:\n",
    "#         # Check if the loss is NaN\n",
    "#         if torch.isnan(loss):\n",
    "#             print(\"NaN loss detected!\")\n",
    "#             print(f\"Input labels at time of NaN loss: {inputs['labels']}\")\n",
    "\n",
    "#         if torch.isnan(outputs.logits).any():\n",
    "#             print(\"NaN values found in logits!\")\n",
    "#             print(f\"Logit values at time of NaN logits: {outputs.logits}\")\n",
    "\n",
    "#         print(f\"Outputs: {outputs}\")\n",
    "#         print(f\"Loss Before Backpropagation: {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        return loss.detach()\n",
    "\n",
    "    def log(self, logs: Dict[str, float], iterator: Optional[Any] = None) -> None:\n",
    "        \"\"\"Scale up loss values before logging\"\"\"\n",
    "        if \"loss\" in logs:\n",
    "            logs[\"loss\"] = logs['loss']\n",
    "        super().log(logs, iterator)\n",
    "\n",
    "class LogMetricsCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None:\n",
    "            log_str = \"Evaluation Metrics: \"\n",
    "            for k, v in metrics.items():\n",
    "                log_str += f\"{k}: {v:.4f}, \"  # Format to 4 decimal places\n",
    "            log_str = log_str[:-2]  # Remove trailing comma and space\n",
    "            logger.info(log_str)\n",
    "        else:\n",
    "            logger.info(\"No metrics available at this evaluation step.\")\n",
    "\n",
    "class FinancialPredictor:\n",
    "    def __init__(self, config: ModelConfig, model_path=None):\n",
    "        self.config = config\n",
    "        self.device = setup_environment()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "        self.model = None\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Loads a pretrained model and its tokenizer\"\"\"\n",
    "        try:\n",
    "            self.model = ModernBertForSequenceClassification.from_pretrained(\n",
    "                model_path,\n",
    "                num_labels=self.config.num_labels,\n",
    "                problem_type=\"regression\",\n",
    "                torch_dtype=torch.bfloat16\n",
    "            )\n",
    "            # Load the LoRA adapter\n",
    "            config = PeftConfig.from_pretrained(model_path)\n",
    "            self.model = PeftModel.from_pretrained(self.model, model_path, config=config, torch_dtype=torch.bfloat16).to(self.device)\n",
    "            logger.info(f\"Successfully loaded model and adapter from {model_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model from {model_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        \n",
    "    def prepare_data(self, df: pd.DataFrame):\n",
    "        required_columns = ['labels', 'snapshot_file', 'year']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "        df = df.dropna(subset=['snapshot_file', 'year'])\n",
    "        return split_data_by_labels(df)\n",
    "        \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        df_train_val, _, _ = self.prepare_data(df)\n",
    "        df = df_train_val.sort_values(by=['year', 'q', 'cik'], ascending=[True, True, True])\n",
    "\n",
    "        splits = rolling_window_split(\n",
    "            df_train_val,\n",
    "            train_window_years=5,  # Use 5 years for training\n",
    "            validation_years=1,    # Validate on the next year\n",
    "            min_train_years=3,     # Require at least 3 years of training data\n",
    "            stride=1              # Move forward 1 year at a time\n",
    "        )\n",
    "\n",
    "        \n",
    "        if not splits:\n",
    "            raise ValueError(\"No valid train/validation splits created\")\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            warmup_ratio=self.config.warmup_ratio,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=200,\n",
    "            eval_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=3,\n",
    "            report_to=\"wandb\",\n",
    "            metric_for_best_model=\"avg_rmse\",\n",
    "            greater_is_better=False,\n",
    "            bf16=True,\n",
    "            torch_compile=False,\n",
    "        )\n",
    "\n",
    "        for i, (train_df, val_df) in enumerate(splits):\n",
    "            logger.info(f\"Training on split {i+1}/{len(splits)}\")\n",
    "            logger.info(f\"Train years: {sorted(train_df['year'].unique())}\")\n",
    "            logger.info(f\"Validation years: {sorted(val_df['year'].unique())}\")\n",
    "            \n",
    "            self.model = create_model(self.config)\n",
    "            \n",
    "            train_dataset = DynamicTextDataset(train_df, self.tokenizer, self.config.max_length)\n",
    "            \n",
    "            if val_df['labels'].isnull().any().any():  # Check for NaNs in labels\n",
    "                logger.error(f\"Validation DataFrame for split {i} contains NaNs in labels!\")\n",
    "                # Optionally, print the rows with NaNs for further inspection\n",
    "                logger.error(val_df[val_df['labels'].isnull().any(axis=1)])\n",
    "                raise ValueError(\"NaNs found in validation labels\")\n",
    "\n",
    "\n",
    "            val_dataset = DynamicTextDataset(val_df, self.tokenizer, self.config.max_length)\n",
    "            \n",
    "            trainer = ScaledLossTrainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                data_collator=collate_fn,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=3), LogMetricsCallback]\n",
    "            )\n",
    "            \n",
    "            trainer.train()\n",
    "            \n",
    "            output_dir = f\"./fine_tuned_model_split_{i}\"\n",
    "            self.model.save_pretrained(output_dir)\n",
    "            \n",
    "            wandb.log({\n",
    "                f\"split_{i}_train_years\": sorted(train_df['year'].unique()),\n",
    "                f\"split_{i}_val_years\": sorted(val_df['year'].unique()),\n",
    "                f\"split_{i}_final_metrics\": trainer.state.best_metric\n",
    "            })\n",
    "        \n",
    "        return trainer\n",
    "\n",
    "    def predict(self, df: pd.DataFrame, batch_size: int = 2) -> pd.DataFrame:\n",
    "        df = get_latest_files(df)\n",
    "        dataset = DynamicTextDataset(df, self.tokenizer, self.config.max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "        \n",
    "        predictions = []\n",
    "        metadata_list = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                outputs = self.model(\n",
    "                    input_ids=batch['input_ids'].to(self.device),\n",
    "                    attention_mask=batch['attention_mask'].to(self.device, dtype=torch.bfloat16)\n",
    "                )\n",
    "                predictions.extend(outputs.logits.cpu().float().numpy())\n",
    "                metadata_list.extend(batch['metadata'])\n",
    "        \n",
    "        df['predictions'] = predictions\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def predict_and_rank(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Combine prediction and ranking in one step\"\"\"\n",
    "        predictions_df = self.predict(df)\n",
    "        ranker = FinancialRanking()\n",
    "        ranked_df = ranker.rank_stocks(predictions_df)\n",
    "        return ranked_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27baf5-26c0-4e6e-acb6-09b22ce717df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970672e2-fd43-4134-850e-8eaec4981680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_latest_files(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Get the most recent file for each CIK\"\"\"\n",
    "    return df.sort_values(['cik', 'year', 'q']).drop_duplicates('cik', keep='last')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe91223-0e36-4fd3-999f-5c69ecdf3baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe71c6a2-d1c6-45bb-8a38-5a32f50d00a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"modernbert-finetuning-lora-rolling-window\", \n",
    "    entity=os.environ['WANDB_ENTITY'],\n",
    "    settings=wandb.Settings(init_timeout=120),\n",
    "    config={\n",
    "        \"model\": \"answerdotai/modernbert-large\",\n",
    "        \"max_length\": 3300,\n",
    "        \"batch_size\": 3,\n",
    "        \"learning_rate\": 1e-4\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93d474-ec3d-4f91-beee-99de591236ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config = ModelConfig()\n",
    "print(\"Configuration loaded successfully\")\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data from parquet file...\")\n",
    "df = pd.read_parquet('training_data.parquet')\n",
    "print(f\"Loaded dataset with {len(df)} rows\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70398ac-825b-4aca-b971-50aad6053622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize predictor\n",
    "# predictor = FinancialPredictor(config)\n",
    "# Initialize predictor\n",
    "# Find the latest saved model directory\n",
    "model_path = None\n",
    "list_of_files = os.listdir(\"./\")\n",
    "fine_tuned_dirs = [f for f in list_of_files if f.startswith(f\"fine_tuned_{config.base_model.split('/')[1]}_split_\")]\n",
    "if fine_tuned_dirs:\n",
    "    model_path = sorted(fine_tuned_dirs, key=lambda f: int(f.split(\"_\")[-1]))[-1]\n",
    "    print(f\"Loading latest saved model from: {model_path}\")\n",
    "else:\n",
    "    model_path = \"./final_model\"\n",
    "    print(f\"Using default model path: {model_path}\")\n",
    "\n",
    "predictor = FinancialPredictor(config, model_path=model_path) # Uncomment if you want to load an already trained model\n",
    "\n",
    "# Split data\n",
    "print(\"Splitting data based on label availability...\")\n",
    "df_train_val, df_some_null, df_all_null = predictor.prepare_data(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d0309-d87c-4534-9dfd-5b2948647393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your labels are lists/arrays of floats\n",
    "all_labels = [item for sublist in df_train_val['labels'] for item in sublist]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(all_labels, kde=True)\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label Statistics:\")\n",
    "print(f\"Mean: {np.mean(all_labels)}\")\n",
    "print(f\"Median: {np.median(all_labels)}\")\n",
    "print(f\"Standard Deviation: {np.std(all_labels)}\")\n",
    "print(f\"Min: {np.min(all_labels)}\")\n",
    "print(f\"Max: {np.max(all_labels)}\")\n",
    "print(f\"Number of zero labels: {len(list(filter(lambda x: x == 0, all_labels)))}\")\n",
    "\n",
    "# Check for positive infinity\n",
    "inf_labels_count = np.isinf(all_labels).sum()\n",
    "print(f\"Number of positive infinity labels: {inf_labels_count}\")\n",
    "\n",
    "# Check for negative infinity\n",
    "neg_inf_labels_count = np.isneginf(all_labels).sum()\n",
    "print(f\"Number of negative infinity labels: {neg_inf_labels_count}\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edebad-1765-4528-b602-f201812725de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_length(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return len(f.read())\n",
    "    \n",
    "df_train_val['text_length'] = df_train_val['snapshot_file'].apply(get_text_length)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_train_val['text_length'], kde=True)\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(df_train_val['text_length'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9b9ae-cd16-4488-b734-2e8f59ad242a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_length_indices = df_train_val[df_train_val['text_length'] == 0].index\n",
    "\n",
    "# Remove rows with zero-length files in place\n",
    "df_train_val.drop(zero_length_indices, inplace=True)\n",
    "\n",
    "# Reset the index if needed\n",
    "df_train_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Number of zero-length files removed: {len(zero_length_indices)}\")\n",
    "\n",
    "# It's good practice to remove the temporary 'text_length' column\n",
    "df_train_val.drop(columns=['text_length'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09d173-5aaf-4e3c-a5ac-85cc5a51d3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f802a-03e5-456c-a81e-63cf61721794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_some_null[df_some_null['labels'].apply(lambda labels: pd.isna(labels[2:]).all())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb786d41-de6f-4a64-9ee9-49cf840f8c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log data split information\n",
    "wandb.log({\n",
    "    \"train_val_count\": len(df_train_val),\n",
    "    \"some_null_count\": len(df_some_null),\n",
    "    \"all_null_count\": len(df_all_null),\n",
    "    \"total_samples\": len(df)\n",
    "})\n",
    "\n",
    "print(f\"\"\"\n",
    "Data split summary:\n",
    "- Training/Validation set: {len(df_train_val)} samples\n",
    "- Partial labels set: {len(df_some_null)} samples\n",
    "- No labels set: {len(df_all_null)} samples\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125639f-6df0-4510-ac94-a14e4ad07012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "trainer = predictor.train(df_train_val)\n",
    "print(\"Model training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfcfb86-1e8b-495e-983c-1583bbf581eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "min_year = 2024\n",
    "combined_df = pd.concat([df_some_null, df_all_null], ignore_index=True)\n",
    "\n",
    "# Filter for recent years\n",
    "recent_df = combined_df[combined_df['year'] & combined_df['labels'].apply(lambda labels: pd.isna(labels[2:]).all())]\n",
    "recent_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e804c30-71b5-433c-bfa9-61fc80d43e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get most recent snapshot for each company\n",
    "latest_snapshots = recent_df.sort_values(['cik', 'year', 'q'], ascending=[True, True, True]) \\\n",
    "                           .groupby('cik').last().reset_index()\n",
    "predictions_df = predictor.predict(latest_snapshots)\n",
    "\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189a821-d24a-4ca6-907f-477709af08ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank predictions\n",
    "logger.info(\"Ranking stocks based on predictions...\")\n",
    "ranker = FinancialRanking()\n",
    "ranked_df = ranker.rank_stocks(predictions_df)\n",
    "ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25c25e-def0-41bc-8663-de72a5628b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sec_cik_mapper\n",
    "\n",
    "stock_mapper = sec_cik_mapper.StockMapper()\n",
    "cik_to_tickers = stock_mapper.cik_to_tickers\n",
    "cik_to_name = stock_mapper.cik_to_company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c399-4351-4c71-8d78-e0b7036b3aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ranked_df['tickers'] = ranked_df['cik'].apply(cik_to_tickers.get)\n",
    "ranked_df['name'] = ranked_df['cik'].apply(cik_to_name.get)\n",
    "pd.set_option('display.max_rows', None)\n",
    "ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa7b9b-5600-4a1d-b17b-9d18155f2a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save results\n",
    "output_file = 'ranked_predictions_base.csv'\n",
    "ranked_df.to_csv(output_file, index=False)\n",
    "logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Log final results\n",
    "wandb.log({\n",
    "    \"top_10_stocks\": wandb.Table(dataframe=ranked_df.head(10)),\n",
    "    \"final_model_performance\": trainer.state.best_metric if trainer else None\n",
    "})\n",
    "\n",
    "# Save model artifacts\n",
    "final_model_path = \"./final_model\"\n",
    "predictor.model.save_pretrained(final_model_path)\n",
    "predictor.tokenizer.save_pretrained(final_model_path)\n",
    "wandb.finish()\n",
    "logger.info(\"Weights & Biases logging completed\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
