{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Language Models to Predict Stock Performance\n",
    "This notebook is a relatively basic attempt at using the capabilities provided by language models to predict stock performance based on historical stock performance and macro-economic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install a way to convert between SEC CIK primary keys and TICKR symbols. Leverage the finagg library which aggregates some useful free to access financial data.\n",
    "%pip install --upgrade sec-cik-mapper finagg requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import environ, getcwd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "environ['FINAGG_ROOT_PATH'] = f'{getcwd()}'\n",
    "\"\"\"\n",
    "Prior to running this cell you'll need  a .env file with some api keys.\n",
    "\n",
    "BEA_API_KEY=#get from https://apps.bea.gov/api/signup/\n",
    "FRED_API_KEY=#get fromhttps://fred.stlouisfed.org/docs/api/api_key.html\n",
    "\"\"\"\n",
    "load_dotenv(f'{getcwd()}/env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!finagg install -ss economic -ts sec -ts indices --stock-data -z -r -s sec -s yfinance\n",
    "!finagg fred install --raw series --series observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, os, csv\n",
    "# Set up folder to persist financial data to.\n",
    "fin_data_path = 'findata'\n",
    "os.makedirs(fin_data_path, exist_ok=True)\n",
    "\n",
    "russell_path_raw = f'{fin_data_path}/russell-3000.csv'\n",
    "russell_path = f'{fin_data_path}/russell-3000-clean.csv'\n",
    "\n",
    "url = 'https://www.ishares.com/us/products/239714/ishares-russell-3000-etf/1467271812596.ajax?fileType=csv&fileName=IWV_holdings&dataType=fund&asOfDate=20240321'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "with open(russell_path_raw, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "with open(russell_path_raw, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Get the start and end row of csv\n",
    "empty_row_indicies = [i for i in range(len(rows)) if (len(rows[i]) == 0 or '\\xa0' in rows[i])]\n",
    "\n",
    "print('Empty rows:', empty_row_indicies)\n",
    "\n",
    "start = empty_row_indicies[0] + 1\n",
    "end = empty_row_indicies[1]\n",
    "# Skip rows with ticker symbols and skip irrelevant file metadata. Only include NASDAQ and NYSE exchanges.\n",
    "relevant_rows = [ r for r in rows[start:end] if r[0] != '-' and r[10].strip() in ('Exchange', 'NASDAQ', 'New York Stock Exchange Inc.') ]\n",
    "\n",
    "# write csv\n",
    "with open(russell_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(relevant_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install pandas so we can work with dataframes\n",
    "%pip install --upgrade pandas pandas-ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load Russell 3000 holdings CSV into a dataframe\n",
    "holdings = pd.read_csv(russell_path)\n",
    "holdings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "holdings.drop(columns=['Market Value', 'Weight (%)', 'Notional Value', 'Price', 'FX Rate', 'Currency', 'Market Currency', 'Accrual Date', 'Asset Class'], inplace=True)\n",
    "holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the sec_cik_mapper module to help us find financial data associated with above companies\n",
    "import sec_cik_mapper\n",
    "\n",
    "stock_mapper = sec_cik_mapper.StockMapper()\n",
    "ticker_to_cik = stock_mapper.ticker_to_cik\n",
    "cik_to_exchange = stock_mapper.cik_to_exchange\n",
    "cik_to_name = stock_mapper.cik_to_company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "holdings['cik'] = holdings['Ticker'].map(lambda ticker: ticker_to_cik.get(ticker))\n",
    "train_companies = holdings.dropna(subset=['cik'])\n",
    "train_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store output organized in output directories\n",
    "outdir = './out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install numpy dataclasses-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import finagg\n",
    "\n",
    "# Utility methods to get financial indicators:\n",
    "quarter_ranges = {\n",
    "    1: ('01-01', '04-01'),\n",
    "    2: ('04-01', '07-01'),\n",
    "    3: ('07-01', '10-01'),\n",
    "    4: ('10-01', '01-01')\n",
    "}\n",
    "\n",
    "def get_fred_indicator(year, q, indicator):\n",
    "    \"\"\"\n",
    "    Fetches the specified FRED indicator for the given year and quarter.\n",
    "    Caches the results to avoid redundant API calls.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with shelve.open(\"fred-indicators\") as indicator_cache:\n",
    "            year, q = int(year), int(q)\n",
    "            indicator_cache_key = \"|\".join([str(year), str(q), indicator])\n",
    "            if indicator_cache_key in indicator_cache:\n",
    "                return indicator_cache[indicator_cache_key]\n",
    "            \n",
    "            # Determine the end year based on the quarter\n",
    "            end_year = year if q < 4 else year + 1\n",
    "            \n",
    "            # Fetch data from FRED API\n",
    "            data = finagg.fred.api.series.observations.get(\n",
    "                indicator,\n",
    "                observation_start=f'{year}-{quarter_ranges[q][0]}',\n",
    "                observation_end=f'{end_year}-{quarter_ranges[q][1]}',\n",
    "            )\n",
    "            \n",
    "            # Calculate the average for the quarter\n",
    "            result = data['value'].aggregate(lambda l: sum(l) / len(l))\n",
    "            indicator_cache[indicator_cache_key] = result  # Cache the result\n",
    "            return result\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def get_share_price(ticker, start_date, end_date):\n",
    "    return finagg.yfinance.api.get(ticker, start=str(start_date), end=str(end_date))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade retry timeout_decorator # Some of what we're about to do is unreliable. We install libraries to help us make it reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown, display\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GoogleSearch,\n",
    "    Part,\n",
    "    Retrieval,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    "    VertexAISearch,\n",
    ")\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Prompt(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def text(self) -> dict:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "@dataclass\n",
    "class GeminiPrompt(Prompt):\n",
    "    task: str\n",
    "    context_information: Optional[List[str]] = None\n",
    "    instructions: Optional[List[str]] = None\n",
    "    response_format_instructions: Optional[List[str]] = None\n",
    "\n",
    "    \n",
    "    def text(self) -> dict:\n",
    "        \"\"\"\n",
    "        According to nova [guidance](https://docs.aws.amazon.com/nova/latest/userguide/prompting-precision.html), response format instructions should be in the form:\n",
    "        // use this to clearly define the task and job needed by the model\n",
    "        Task:\n",
    "        {{Task summary}} \n",
    "\n",
    "        // use this to provide contextual information related to the task\n",
    "        Context information:\n",
    "        - {{Context and content information 1}}\n",
    "        - {{Context and content information 2}}\n",
    "        ...\n",
    "\n",
    "        // use this to provide any model instructions that you want model to adhere to\n",
    "        Model Instructions:\n",
    "        - {{ Other Model Instructions }}\n",
    "        ...\n",
    "\n",
    "        // use this to provide response style and formatting guidance\n",
    "        Response style and format requirements:\n",
    "        - {{Style and format requirement 1}}\n",
    "        - {{Style and format requirement 2}}\n",
    "        ...\n",
    "\n",
    "        \"\"\"\n",
    "        context_information = \"\\n\".join([ f\"- {info}\" for info in self.context_information]) if self.context_information else \"\"\n",
    "        model_instructions = \"\\n\".join([ f\"- {instruction}\" for instruction in self.instructions]) if self.instructions else \"\"\n",
    "        response_format_instructions = \"\\n\".join([ f\"- {instruction}\" for instruction in self.response_format_instructions]) if self.response_format_instructions else \"\"\n",
    "        prompt_text = f\"Task:\\n{self.task}\\n\\n\"\n",
    "        if context_information:\n",
    "            prompt_text += f\"Context information:\\n{context_information}\\n\\n\"\n",
    "        if model_instructions:\n",
    "            prompt_text += f\"Model Instructions:\\n{model_instructions}\\n\\n\"\n",
    "        if response_format_instructions:\n",
    "            prompt_text += f\"Response style and format requirements:\\n{response_format_instructions}\\n\\n\"\n",
    "        return prompt_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "LOCATION = \"us-central1\"\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "MODEL_ID = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "def get_gemini_response(prompt: Prompt,  output_format=\"json\", max_tokens=1000, model_id=MODEL_ID):\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=prompt.text(),\n",
    "        config=GenerateContentConfig(\n",
    "            stop_sequences=[\"\\n```\"],\n",
    "            max_output_tokens=max_tokens\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        text = response.text.replace(f\"```{output_format}\",\"\").replace(\"```\",\"\")\n",
    "        if output_format == \"json\":\n",
    "            return json.loads(text)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse response: {response}.\\n Query was: {prompt.text()}\")\n",
    "        raise\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example prompt that selects 10 keys from the us-gaap keys provided in the context below to enable a financial analyst to quickly but accurately understand the company's performance.\n",
    "\n",
    "keys = ['AccountsPayableAndAccruedLiabilitiesCurrent', 'AccountsReceivableNetCurrent', 'AccruedIncomeTaxesCurrent', 'AccumulatedDepreciationDepletionAndAmortizationPropertyPlantAndEquipment', 'AccumulatedOtherComprehensiveIncomeLossNetOfTax', 'AdditionalPaidInCapitalCommonStock', 'AllowanceForDoubtfulAccountsReceivableCurrent', 'AssetImpairmentCharges', 'Assets', 'AssetsCurrent', 'AssetsOfDisposalGroupIncludingDiscontinuedOperation', 'AvailableForSaleDebtSecuritiesGrossUnrealizedLoss', 'AvailableForSaleSecurities', 'AvailableForSaleSecuritiesAmortizedCost', 'AvailableForSaleSecuritiesDebtMaturitiesAfterFiveThroughTenYearsAmortizedCost', 'AvailableForSaleSecuritiesDebtMaturitiesAfterFiveThroughTenYearsFairValue', 'AvailableForSaleSecuritiesDebtMaturitiesAfterOneThroughFiveYearsAmortizedCost', 'AvailableForSaleSecuritiesDebtMaturitiesAfterOneThroughFiveYearsFairValue', 'AvailableForSaleSecuritiesDebtMaturitiesAfterTenYearsAmortizedCost', 'AvailableForSaleSecuritiesDebtMaturitiesAfterTenYearsFairValue', 'AvailableForSaleSecuritiesDebtMaturitiesWithinOneYearAmortizedCost', 'AvailableForSaleSecuritiesDebtMaturitiesWithinOneYearFairValue', 'AvailableForSaleSecuritiesGrossRealizedGains', 'AvailableForSaleSecuritiesGrossRealizedLosses', 'AvailableForSaleSecuritiesGrossUnrealizedGains', 'AvailableForSaleSecuritiesGrossUnrealizedLoss', 'BusinessCombinationStepAcquisitionEquityInterestInAcquireeRemeasurementLoss', 'CashAndCashEquivalentsAtCarryingValue', 'CashAndCashEquivalentsPeriodIncreaseDecrease', 'CashCashEquivalentsAndShortTermInvestments', 'CashFlowHedgeGainLossToBeReclassifiedWithinTwelveMonths', 'CommonStockDividendsPerShareDeclared', 'CommonStockParOrStatedValuePerShare', 'CommonStockSharesAuthorized', 'CommonStockSharesIssued', 'CommonStockValue', 'ComprehensiveIncomeNetOfTax', 'ComprehensiveIncomeNetOfTaxAttributableToNoncontrollingInterest', 'ComprehensiveIncomeNetOfTaxIncludingPortionAttributableToNoncontrollingInterest', 'CostMethodInvestments', 'CostOfGoodsSold', 'DeferredIncomeTaxExpenseBenefit', 'DeferredTaxLiabilitiesNoncurrent', 'DefinedBenefitPlanContributionsByEmployer', 'DefinedBenefitPlansEstimatedFutureEmployerContributionsInCurrentFiscalYear', 'DepreciationDepletionAndAmortization', 'DerivativeCollateralObligationToReturnCash', 'DisposalGroupNotDiscontinuedOperationGainLossOnDisposal', 'DividendsCommonStockCash', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'EffectiveIncomeTaxRateContinuingOperations', 'EffectiveIncomeTaxRateReconciliationAtFederalStatutoryIncomeTaxRate', 'EffectOfExchangeRateOnCashAndCashEquivalents', 'EquityMethodInvestmentRealizedGainLossOnDisposal', 'EquityMethodInvestments', 'ExtinguishmentOfDebtAmount', 'ForeignCurrencyTransactionGainBeforeTax', 'ForeignCurrencyTransactionGainLossUnrealized', 'GainLossOnSaleOfOtherAssets', 'GainsLossesOnExtinguishmentOfDebt', 'Goodwill', 'GrossProfit', 'ImpairmentOfIntangibleAssetsExcludingGoodwill', 'ImpairmentOfIntangibleAssetsIndefinitelivedExcludingGoodwill', 'IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest', 'IncomeLossFromEquityMethodInvestments', 'IncomeLossFromEquityMethodInvestmentsNetOfDividendsOrDistributions', 'IncomeTaxExpenseBenefit', 'IncreaseDecreaseInOperatingCapital', 'IndefiniteLivedFranchiseRights', 'IndefiniteLivedIntangibleAssetsExcludingGoodwillFairValueDisclosure', 'IndefiniteLivedTrademarks', 'InterestExpense', 'InventoryFinishedGoodsNetOfReserves', 'InventoryNet', 'InventoryRawMaterialsAndSuppliesNetOfReserves', 'InvestmentIncomeInterest', 'LiabilitiesAndStockholdersEquity', 'LiabilitiesCurrent', 'LiabilitiesOfDisposalGroupIncludingDiscontinuedOperation', 'LongTermDebt', 'LongTermDebtCurrent', 'LongTermDebtFairValue', 'LongTermDebtNoncurrent', 'MarketableSecuritiesCurrent', 'MinorityInterest', 'MinorityInterestDecreaseFromDistributionsToNoncontrollingInterestHolders', 'NetCashProvidedByUsedInFinancingActivities', 'NetCashProvidedByUsedInInvestingActivities', 'NetCashProvidedByUsedInOperatingActivities', 'NetIncomeLoss', 'NetIncomeLossAttributableToNoncontrollingInterest', 'NoncontrollingInterestDecreaseFromDeconsolidation', 'OperatingIncomeLoss', 'OtherAssetsNoncurrent', 'OtherComprehensiveIncomeAvailableforsaleSecuritiesAdjustmentBeforeTaxPortionAttributableToParent', 'OtherComprehensiveIncomeAvailableforsaleSecuritiesAdjustmentNetOfTaxPortionAttributableToParent', 'OtherComprehensiveIncomeAvailableforsaleSecuritiesTaxPortionAttributableToParent', 'OtherComprehensiveIncomeDefinedBenefitPlansAdjustmentNetOfTaxPortionAttributableToParent', 'OtherComprehensiveIncomeDerivativesQualifyingAsHedgesNetOfTaxPortionAttributableToParent', 'OtherComprehensiveIncomeForeignCurrencyTransactionAndTranslationAdjustmentNetOfTaxPortionAttributableToParent', 'OtherComprehensiveIncomeForeignCurrencyTransactionAndTranslationGainLossArisingDuringPeriodNetOfTax', 'OtherComprehensiveIncomeForeignCurrencyTransactionAndTranslationGainLossBeforeReclassificationAndTax', 'OtherComprehensiveIncomeForeignCurrencyTranslationGainLossArisingDuringPeriodTax', 'OtherComprehensiveIncomeLossAmortizationAdjustmentFromAOCIPensionAndOtherPostretirementBenefitPlansForNetPriorServiceCostCreditNetOfTax', 'OtherComprehensiveIncomeLossBeforeTax', 'OtherComprehensiveIncomeLossDerivativesQualifyingAsHedgesBeforeTax', 'OtherComprehensiveIncomeLossDerivativesQualifyingAsHedgesNetOfTax', 'OtherComprehensiveIncomeLossDerivativesQualifyingAsHedgesTax', 'OtherComprehensiveIncomeLossForeignCurrencyTransactionAndTranslationAdjustmentBeforeTax', 'OtherComprehensiveIncomeLossForeignCurrencyTransactionAndTranslationAdjustmentNetOfTax', 'OtherComprehensiveIncomeLossForeignCurrencyTransactionAndTranslationReclassificationAdjustmentFromAOCIRealizedUponSaleOrLiquidationBeforeTax', 'OtherComprehensiveIncomeLossForeignCurrencyTransactionAndTranslationReclassificationAdjustmentFromAOCIRealizedUponSaleOrLiquidationNetOfTax', 'OtherComprehensiveIncomeLossForeignCurrencyTransactionAndTranslationReclassificationAdjustmentFromAOCIRealizedUponSaleOrLiquidationTax', 'OtherComprehensiveIncomeLossForeignCurrencyTranslationAdjustmentTax', 'OtherComprehensiveIncomeLossNetOfTax', 'OtherComprehensiveIncomeLossPensionAndOtherPostretirementBenefitPlansAdjustmentBeforeReclassificationAdjustmentsAndTax', 'OtherComprehensiveIncomeLossPensionAndOtherPostretirementBenefitPlansAdjustmentBeforeReclassificationAdjustmentsNetOfTax', 'OtherComprehensiveIncomeLossPensionAndOtherPostretirementBenefitPlansAdjustmentBeforeTax', 'OtherComprehensiveIncomeLossPensionAndOtherPostretirementBenefitPlansAdjustmentNetOfTax', 'OtherComprehensiveIncomeLossPensionAndOtherPostretirementBenefitPlansBeforeReclassificationAdjustmentsTax', 'OtherComprehensiveIncomeLossPensionAndOtherPostretirementBenefitPlansTax', 'OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIForSaleOfSecuritiesBeforeTax', 'OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIForSaleOfSecuritiesNetOfTax', 'OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIForSaleOfSecuritiesTax', 'OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIOnDerivativesBeforeTax', 'OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIOnDerivativesNetOfTax', 'OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIOnDerivativesTax', 'OtherComprehensiveIncomeLossTax', 'OtherComprehensiveIncomeUnrealizedGainLossOnDerivativesArisingDuringPeriodBeforeTax', 'OtherComprehensiveIncomeUnrealizedGainLossOnDerivativesArisingDuringPeriodNetOfTax', 'OtherComprehensiveIncomeUnrealizedGainLossOnDerivativesArisingDuringPeriodTax', 'OtherComprehensiveIncomeUnrealizedHoldingGainLossOnSecuritiesArisingDuringPeriodBeforeTax', 'OtherComprehensiveIncomeUnrealizedHoldingGainLossOnSecuritiesArisingDuringPeriodNetOfTax', 'OtherComprehensiveIncomeUnrealizedHoldingGainLossOnSecuritiesArisingDuringPeriodTax', 'OtherInventoryNetOfReserves', 'OtherLiabilitiesNoncurrent', 'OtherNoncashExpense', 'OtherNoncashIncomeExpense', 'OtherNonoperatingIncomeExpense', 'OtherShortTermInvestments', 'PaymentsForProceedsFromOtherInvestingActivities', 'PaymentsForRepurchaseOfCommonStock', 'PaymentsOfDividends', 'PaymentsToAcquirePropertyPlantAndEquipment', 'PrepaidExpenseAndOtherAssetsCurrent', 'ProceedsFromIssuanceOfCommonStock', 'ProceedsFromIssuanceOfDebt', 'ProceedsFromPaymentsForOtherFinancingActivities', 'ProceedsFromSaleOfAvailableForSaleSecurities', 'ProceedsFromSaleOfPropertyPlantAndEquipment', 'ProfitLoss', 'PropertyPlantAndEquipmentNet', 'RepaymentsOfDebt', 'RepaymentsOfLongTermDebt', 'RetainedEarningsAccumulatedDeficit', 'SalesRevenueGoodsNet', 'SellingGeneralAndAdministrativeExpense', 'ShareBasedCompensation', 'StockholdersEquity', 'StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest', 'StockIssuedDuringPeriodValueShareBasedCompensation', 'TreasuryStockShares', 'TreasuryStockValue', 'TreasuryStockValueAcquiredCostMethod', 'WeightedAverageNumberDilutedSharesOutstandingAdjustment', 'WeightedAverageNumberOfDilutedSharesOutstanding', 'WeightedAverageNumberOfSharesOutstandingBasic']\n",
    "key_string = '\\n' + '\\n* '.join(keys)\n",
    "context_information = [f\"In this companies us-gaap report, the following keys are available: {key_string}\"]\n",
    "prompt = GeminiPrompt(\n",
    "    task=\"Select 20 keys from the us-gaap keys provided in the context to enable a financial analyst to quickly make a snap-judgment of the company's performance.\",\n",
    "    context_information=context_information,\n",
    "    instructions=[\"Be thorough in your analysis and ensure the keys you select are categorical, distinct, non-overlapping and represent key financial indicators like profits, revenues, assets, liabilities, and investments in company growth. Ensure the keys summarize the company's financial health effectively and are present in the context information.\"],\n",
    "    response_format_instructions=[\"Respond with only a markdown code block containing a json list of strings representing the selected keys: eg. ```json\\n[\\nKey1,\\nKey2\\n,...,Key10\\n]\\n```\", \"Verify the presence of each of the keys in the provided context before responding\", \"Preserve the original PascalCase of the keys eg. 'AssetsCurrent' rather than 'Assets Current' and 'CostOfGoodsSold' instead of 'Cost Of Goods Sold'\", \"Order the keys in the order of importance\"]\n",
    ")\n",
    "\n",
    "response = get_gemini_response(prompt, output_format=\"json\")\n",
    "\n",
    "print(response)\n",
    "for key in response:\n",
    "    try:\n",
    "        assert key in keys\n",
    "    except AssertionError:\n",
    "        print(f\"Key {key} not found in context information\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "from retry import retry\n",
    "from timeout_decorator import timeout\n",
    "\n",
    "@retry(tries=4, delay=15)\n",
    "@timeout(300)\n",
    "def most_relevant_keys(keys, n, prompt_cache):\n",
    "    key_string = '\\n* '.join(keys)\n",
    "    context_information = [f\"In this companies us-gaap report, the following keys are available:\\n {key_string}\"]\n",
    "    prompt = GeminiPrompt(\n",
    "        task=f\"Select {n} keys from the us-gaap keys provided in the context to enable a financial analyst to quickly make a snap-judgment of the company's performance.\",\n",
    "        context_information=context_information,\n",
    "        instructions=[\"Be thorough in your analysis\", \"Ensure the keys you select are categorical, distinct, non-overlapping and represent key financial indicators like profits, revenues, assets, liabilities, and investments in company growth.\", \"Ensure the keys summarize the company's financial health effectively and are present in the context information.\"],\n",
    "        response_format_instructions=[\"Respond with only a markdown code block containing a json list of strings representing the selected keys: eg. ```json\\n[\\nKey1,\\nKey2\\n,...,Key10\\n]\\n```\", \"Verify the presence of each of the keys in the provided context before responding\", \"Preserve the original PascalCase of the keys eg. 'AssetsCurrent' rather than 'Assets Current' and 'CostOfGoodsSold' instead of 'Cost Of Goods Sold'\", \"Order the keys in the order of importance\"]\n",
    "    )\n",
    "    hash = md5(''.join(sorted(keys)).encode('utf-8')).hexdigest()\n",
    "    if hash in prompt_cache:\n",
    "        return prompt_cache[hash]\n",
    "    result = get_gemini_response(prompt, output_format=\"json\")\n",
    "    prompt_cache[hash] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PointInTimeValue:\n",
    "    q: int\n",
    "    year: int\n",
    "    value: int | float | str\n",
    "    unit: str\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# Regex to find <DOCUMENT> tags\n",
    "doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "# Regex to find <TYPE> tag prceeding any characters, terminating at new line\n",
    "type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import io\n",
    "from io import StringIO\n",
    "\n",
    "@dataclass\n",
    "class Form10KExtracts:\n",
    "    risk_factors: str\n",
    "    md_and_a: str\n",
    "    disclosures: str\n",
    "    hash_digest: str\n",
    "    \n",
    "@dataclass\n",
    "class Form10K:\n",
    "    file: str\n",
    "    year: str\n",
    "    q: str  \n",
    "    \n",
    "def clean_table_dataframe(table_html):\n",
    "    # Parse the table with pandas\n",
    "    df = pd.read_html(table_html)[0]\n",
    "    \n",
    "    # Drop completely empty rows and columns\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    # Clean up column names: combine multi-row headers, if any\n",
    "    df.columns = df.iloc[0].fillna('')  # Use the first row as column names\n",
    "    df = df.drop(index=0)  # Drop the first row, now that it's used as headers\n",
    "    \n",
    "    # Remove any rows with just empty values or text (e.g., notes, references)\n",
    "    df = df[~df.apply(lambda row: row.str.contains(r'[a-zA-Z]').any(), axis=1)]\n",
    "    \n",
    "    # Clean up any extraneous characters or symbols in numeric columns\n",
    "    def clean_numeric(val):\n",
    "        try:\n",
    "            # Remove commas and other non-numeric symbols, then convert to float\n",
    "            return pd.to_numeric(str(val).replace(',', '').replace('(', '-').replace(')', ''), errors='coerce')\n",
    "        except:\n",
    "            return val\n",
    "\n",
    "    df = df.applymap(clean_numeric)\n",
    "    \n",
    "    # Further clean if the table has extra empty cells after data rows\n",
    "    df = df.dropna(how='all', axis=1)  # Drop columns where all values are NaN\n",
    "    \n",
    "    # Reset the index to have a clean row index after modifications\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def get_text_only(html_fragment):\n",
    "    \"\"\"\n",
    "    Extracts readable text while preserving tables from an HTML fragment.\n",
    "    Processes tables to remove noise and converts them to CSV-like strings.\n",
    "    \n",
    "    Args:\n",
    "        html_fragment (str): The HTML content to parse.\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text with cleaned tables as CSV-like strings.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_fragment, \"html.parser\")\n",
    "\n",
    "    # Extract and clean tables\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        try:\n",
    "            # Parse the table using pandas and clean it\n",
    "            df = pd.read_html(StringIO(table))[0]  # Parse the first table\n",
    "            cleaned_df = clean_table_dataframe(df)  # Clean the table data\n",
    "            table_text = cleaned_df.to_csv(index=False, date_format='%Y-%m-%d')  # Convert to CSV string\n",
    "            table.replace_with(f\"\\n[Table]\\n{table_text}\\n[/Table]\\n\")  # Replace table with CSV-like text\n",
    "        except Exception as e:\n",
    "            # Fallback to raw CSV (unprocessed) if cleaning fails\n",
    "            # Get the raw table data as CSV using pandas' to_csv method\n",
    "            try:\n",
    "                raw_csv = pd.read_html(StringIO(str(table)), header=None)[0].to_csv(index=False, header=False, date_format='%Y-%m-%d')\n",
    "                table.replace_with(f\"\\n[Table]\\n{raw_csv}\\n[/Table]\\n\")\n",
    "            except Exception as inner_e:\n",
    "                table.replace_with(f\"\\n[Table]\\nError processing table: {str(inner_e)}\\n[/Table]\\n\")\n",
    "\n",
    "    # Extract plain text from the remaining HTML content\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    # Remove excessive empty lines and whitespace\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def clean_string(input_text):\n",
    "    \"\"\"\n",
    "    Remove excessive empty lines from a string.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): Input text with potentially excessive whitespace\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text with reduced whitespace\n",
    "    \"\"\"\n",
    "    input_text = re.sub(r'<[^\\>]+?\\s*$', '', input_text)\n",
    "    lines = input_text.splitlines()\n",
    "    \n",
    "    # Clean the lines\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Strip trailing and leading whitespace\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        # Only add non-empty lines or single empty lines\n",
    "        if stripped_line or (not cleaned_lines or cleaned_lines[-1].strip()):\n",
    "            cleaned_lines.append(line.rstrip())\n",
    "    \n",
    "    # Rejoin the lines, adding a single newline between blocks\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "def parse_10_k(text, hash_digest) -> Form10KExtracts | None:\n",
    "    \"\"\"\n",
    "    Parse the 10-K report text to extract the risk factors, md and a, and disclosures sections.\n",
    "    \"\"\"\n",
    "    # regex to find <TYPE> tags followed by section names like '10-K'   \n",
    "    sections_regex = re.compile(r'(>(Item|ITEM)(\\s|&#160;|&nbsp;)(1A|1B|7A|7|8)\\.{0,1})')\n",
    "    matches = sections_regex.finditer(text)\n",
    "    matches_list = [(x.group(), x.start(), x.end()) for x in matches]\n",
    "\n",
    "    # Check if we have any matches before creating DataFrame\n",
    "    if matches_list:\n",
    "        sections_df = pd.DataFrame(matches_list, columns=['item', 'start', 'end'])\n",
    "        sections_df.columns = ['item', 'start', 'end']\n",
    "        sections_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "        sections_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "        sections_df.replace(' ','',regex=True,inplace=True)\n",
    "        sections_df.replace('\\\\.','',regex=True,inplace=True)\n",
    "        sections_df.replace('>','',regex=True,inplace=True)\n",
    "        sections_df['item'] = sections_df.item.str.lower()\n",
    "        sections_df.sort_values('start', ascending=True, inplace=True)\n",
    "        deduped = sections_df.drop_duplicates(subset=['item'], keep='last')\n",
    "        deduped.set_index('item', inplace=True)\n",
    "        risk_factors = clean_string(get_text_only(text[deduped['start'].loc['item1a']:deduped['start'].loc['item1b']]))\n",
    "        md_and_a = clean_string(get_text_only(text[deduped['start'].loc['item7']:deduped['start'].loc['item7a']]))\n",
    "        disclosures = clean_string(get_text_only(text[deduped['start'].loc['item7a']:deduped['start'].loc['item8']]))\n",
    "\n",
    "        return Form10KExtracts(risk_factors, md_and_a, disclosures, hash_digest)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_quarterly_report(file):\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "        doc_start_indexes = [x.end() for x in doc_start_pattern.finditer(text)]\n",
    "        doc_end_indexes = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "\n",
    "        ### Type filter is interesting, it looks for <TYPE> with Not flag as new line, ie terminare there, with + sign\n",
    "        ### to look for any char afterwards until new line \\n. This will give us <TYPE> followed Section Name like '10-K'\n",
    "        ### Once we have have this, it returns String Array, below line will with find content after <TYPE> ie, '10-K' \n",
    "        ### as section names\n",
    "        doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(text)]\n",
    "        parsed_document = {}\n",
    "        for doc_type, doc_start, doc_end in zip(doc_types, doc_start_indexes, doc_end_indexes):\n",
    "            if doc_type == '10-K' or doc_type == '10-Q':\n",
    "                parsed_document[doc_type] = text[doc_start:doc_end].replace(\"\\xa0\", \" \")     \n",
    "        return parsed_document\n",
    "\n",
    "def save_file(parsed_document, file):\n",
    "    with open(file, 'w') as f:\n",
    "       json.dump(parsed_document, f)\n",
    "\n",
    "redaction_instructions = \"Any time the company's name, or year of the report, or any revealing product name would appear in the returned markdown document, redact it using the tag [REDACTED] so that a reader would not know which company is described. Also ensure summaries and quotes do not include names of individuals associated with the company.\"\n",
    "@retry(tries=2, delay=15)\n",
    "@timeout(300)\n",
    "def summarize_risk_factors(risk_factors: str):\n",
    "    \n",
    "    context_information = [f\"The following text was scraped from the risk factors section of a company's 10-K report: ```\\n{risk_factors}\\n```\\n \"]\n",
    "    task = \"Return a three paragraph summary of the most important information for a financial analyst to understand the company's risk factors. Also include a set of up to ten quotes from the text that support the summary.\"\n",
    "    response_format_instructions = [f\"The summary should be formatted as a markdown file with a 'Risk Factors' heading with two sections: Summary and Substantiating Quotes. {redaction_instructions}\", \"Respond only with a markdown code block containing markdown content within starting '```markdown\\n' and ending: '\\n```'\"]\n",
    "    prompt = GeminiPrompt(\n",
    "        task=task,\n",
    "        context_information=context_information,\n",
    "        response_format_instructions=response_format_instructions\n",
    "    )\n",
    "    response = get_gemini_response(prompt, output_format=\"markdown\")\n",
    "    return response\n",
    "\n",
    "@retry(tries=2, delay=15)\n",
    "@timeout(300)\n",
    "def summarize_md_and_a(md_and_a: str):\n",
    "\n",
    "    context_information = [f\"The following text was scraped from the md and a section of a company's 10-K report: ```\\n{md_and_a}\\n```\\n \"]\n",
    "    task = \"Return a three paragraph summary of the most important information for a financial analyst to understand the company's management discussion and analysis. Also include a set of up to ten quotes from the text that support the summary. \"\n",
    "    response_format_instructions = [f\"The summary should be formatted as a markdown file with a 'Management's Discussion and Analysis' heading with two sections: Summary and Substantiating Quotes. {redaction_instructions}\", \"Respond only with a markdown code block containing markdown content within starting '```markdown\\n' and ending: '\\n```'\"]\n",
    "    prompt = GeminiPrompt(\n",
    "        task=task,\n",
    "        context_information=context_information,\n",
    "        response_format_instructions=response_format_instructions\n",
    "    )\n",
    "    response = get_gemini_response(prompt, output_format=\"markdown\")\n",
    "    return response\n",
    "\n",
    "@retry(tries=2, delay=15)\n",
    "@timeout(300)\n",
    "def summarize_disclosures(disclosures: str):\n",
    "    context_information = [f\"The following text was scraped from the disclosures section of a company's 10-K report: ```\\n{disclosures}\\n```\\n \"]\n",
    "    task = \"Return a three paragraph summary of the most important information for a financial analyst to understand the company's disclosures. Also include a set of up to ten quotes from the text that support the summary. \"\n",
    "    response_format_instructions = [f\"The summary should be formatted as a markdown file with a 'Disclosures' heading with two sections: Summary and Substantiating Quotes. {redaction_instructions}\",  \"Respond only with a markdown code block containing markdown content within starting '```markdown\\n' and ending: '\\n```'\"]\n",
    "    prompt = GeminiPrompt(\n",
    "        task=task,\n",
    "        context_information=context_information,\n",
    "        response_format_instructions=response_format_instructions\n",
    "    )\n",
    "    response = get_gemini_response(prompt, output_format=\"markdown\")\n",
    "    return response\n",
    "\n",
    "def summarize_10k_extracts(extracts: Form10KExtracts) -> str:\n",
    "    if extracts is None:\n",
    "        return \"\"\n",
    "    return f\"\"\"\n",
    "{summarize_risk_factors(extracts.risk_factors)}\n",
    "\n",
    "{summarize_md_and_a(extracts.md_and_a)}\n",
    "\"\"\"\n",
    "# \"\"\"\n",
    "# # Disclosures:\n",
    "# {summarize_disclosures(extracts.disclosures)}\n",
    "# \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_macro_metrics(year, q):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of macroeconomic metrics for the given year and quarter.\n",
    "    Includes various FRED indicators.\n",
    "    \"\"\"\n",
    "    # List of additional indicators to fetch\n",
    "    indicators = {\n",
    "        'CPI': 'CPIAUCSL',  # Consumer Price Index\n",
    "        'UnemploymentRate': 'UNRATE',  # Unemployment Rate\n",
    "        'InterestRate': 'GS1',  # 1-Year Treasury Rate\n",
    "        'RetailSales': 'RSAFS',  # Retail Sales\n",
    "        'IndustrialProduction': 'INDPRO',  # Industrial Production Index\n",
    "        'CapacityUtilization': 'TCU',  # Capacity Utilization\n",
    "        'ProducerPriceIndex': 'PPIACO',  # Producer Price Index\n",
    "        'HousingStarts': 'HOUST',  # Housing Starts\n",
    "        'ConsumerSentiment': 'UMCSENT',  # Consumer Sentiment Index\n",
    "        'CorporateBondSpread': 'BAA10YM',  # Corporate Bond Spread (BAA - 10Y Treasury)\n",
    "        'TradeBalance': 'BOPGSTB',  # Trade Balance (Goods and Services)\n",
    "        'AverageHourlyEarnings': 'CES0500000003',  # Average Hourly Earnings (All Employees)\n",
    "    }\n",
    "\n",
    "    # Fetch all requested indicators\n",
    "\n",
    "    indicators = {key: get_fred_indicator(year, q, fred_id) for key, fred_id in indicators.items()}\n",
    "    point_in_time_values = {key: PointInTimeValue(q=q, year=year, value=indicators[key], unit='') for key in indicators}\n",
    "    return point_in_time_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import hashlib\n",
    "from dataclasses_json import dataclass_json\n",
    "from typing import Generator, List, Dict, NamedTuple, Optional, Set, Tuple, Any\n",
    "from enum import Enum\n",
    "import traceback\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "import math\n",
    "import shelve\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FinancialSnapshot:\n",
    "    year: int\n",
    "    q: int\n",
    "    company: 'Company'\n",
    "    financial_info: Dict[str, int | str]\n",
    "\n",
    "@dataclass\n",
    "class Company:\n",
    "    cik: str\n",
    "    ticker: str\n",
    "    exchange: str\n",
    "    name: str\n",
    "    shares: int\n",
    "    sector: str\n",
    "    location: str\n",
    "\n",
    "@dataclass\n",
    "class Label:\n",
    "    stock_price_pre_earnings: float\n",
    "    stock_price_post_earnings: float\n",
    "\n",
    "class Trend(NamedTuple):\n",
    "    two_years_ago: float | int | str | None\n",
    "    one_year_ago: float | int | str | None\n",
    "    nine_months_ago: float | int | str | None\n",
    "    six_months_ago: float | int | str | None\n",
    "    last_quarter: float | int | str | None\n",
    "    current: float | int | str | None\n",
    "\n",
    "class Projection(NamedTuple):\n",
    "    next_quarter: Label\n",
    "    next_six_months: Label\n",
    "    next_year: Label\n",
    "\n",
    "Year = int\n",
    "Quarter = int\n",
    "\n",
    "HistoricalTable = dict[tuple[Year, Quarter], int | float | str | None]\n",
    "\n",
    "FP_TO_Q = {\n",
    "    \"Q1\": 1,\n",
    "    \"Q2\": 2,\n",
    "    \"Q3\": 3,\n",
    "    \"FY\": 4\n",
    "}\n",
    "\n",
    "DATE_TO_Q = {\n",
    "    '03-31': 1,\n",
    "    '06-30': 2,\n",
    "    '09-30': 3,\n",
    "    '12-31': 4\n",
    "}\n",
    "\n",
    "\n",
    "def to_point_in_time(u, vals) -> PointInTimeValue:\n",
    "    try:\n",
    "        end = vals[\"end\"]\n",
    "        year, q = vals[\"fy\"] if vals.get(\"fy\") else int(end[:4]), FP_TO_Q[vals[\"fp\"]] if vals.get(\"fp\") else DATE_TO_Q[end[5:]]\n",
    "        return PointInTimeValue(unit=u, q=q, year=year, value=vals[\"val\"])\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "@retry(tries=4, backoff=2, delay=2)\n",
    "def get_report_share_prices(company: Company, p):\n",
    "    file_year, file_month, file_day = tuple(p[\"filed\"].split('-'))\n",
    "    filing_date = datetime.date(int(file_year), int(file_month), int(file_day))\n",
    "    with shelve.open('share_prices') as share_prices:\n",
    "        ticker, pre_earnings_date, post_earnings_date = company.ticker, filing_date - timedelta(days=1), filing_date + timedelta(days=5)  # use 5 days after closing to smooth out filing spikes\n",
    "        key = f'{ticker}|{pre_earnings_date}|{post_earnings_date}'\n",
    "        if key in share_prices:\n",
    "            return share_prices[key]\n",
    "        else:\n",
    "            share_price = get_share_price(company.ticker, pre_earnings_date, post_earnings_date)\n",
    "            share_price_pre_filing = PointInTimeValue(\n",
    "                unit='USD', \n",
    "                q=FP_TO_Q[p[\"fp\"]], \n",
    "                year=p[\"fy\"], \n",
    "                value=share_price[\"close\"].iloc[0]\n",
    "            )\n",
    "            share_price_post_filing = PointInTimeValue(\n",
    "                unit='USD', \n",
    "                q=FP_TO_Q[p[\"fp\"]], \n",
    "                year=p[\"fy\"], \n",
    "                value=share_price[\"close\"].iloc[-1]\n",
    "            )\n",
    "            share_prices[key] = (share_price_pre_filing, share_price_post_filing)\n",
    "            return share_price_pre_filing, share_price_post_filing\n",
    "\n",
    "\n",
    "def create_report_url(cik, row):\n",
    "    accession_number = row[\"accessionNumber\"].replace(\"-\", \"\")\n",
    "    primary_document = row[\"primaryDocument\"]\n",
    "    url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}/{primary_document}\"\n",
    "    return url\n",
    "\n",
    "def get_report_year(row):\n",
    "    reportDate = row['reportDate']\n",
    "    return reportDate[:reportDate.find('-')]\n",
    "\n",
    "def get_report_q(row):\n",
    "    reportDate = row['reportDate']\n",
    "    return DATE_TO_Q.get(reportDate[reportDate.find('-') + 1:], None)\n",
    "\n",
    "EXCLUDE_KEYS = ['QuarterlyReportAccessionNumber', 'QuarterlyReportUrl']\n",
    "@dataclass\n",
    "class ContextualSnapshot:\n",
    "    year: int\n",
    "    q: int\n",
    "    company: Company\n",
    "    historical_trends: Dict[str, Trend]\n",
    "    future_projection: Projection\n",
    "    most_recent_10k_file: Optional[str] = None\n",
    "\n",
    "    # We are trying to predict the % change in the stock price over the next quarter, six months, and year\n",
    "    def get_labels(self):\n",
    "        pre_earnings_future_values = [\n",
    "            self.future_projection.next_quarter.stock_price_pre_earnings,\n",
    "            self.future_projection.next_six_months.stock_price_pre_earnings,\n",
    "            self.future_projection.next_year.stock_price_pre_earnings\n",
    "        ]\n",
    "        post_earnings_future_values = [\n",
    "            self.future_projection.next_quarter.stock_price_post_earnings,\n",
    "            self.future_projection.next_six_months.stock_price_post_earnings,\n",
    "            self.future_projection.next_year.stock_price_post_earnings\n",
    "        ]\n",
    "        # Use historical trend's current value as the starting point\n",
    "        share_price_pre_earnings = self.historical_trends['SharePricePreFiling'].current.value.astype(float)\n",
    "        share_price_post_earnings = self.historical_trends['SharePricePostFiling'].current.value.astype(float)\n",
    "        # Calculate the change for each of the next quarter, next six months, and year compared to the starting point\n",
    "        pre_earnings_percent_changes = [\n",
    "            (future_value.value - share_price_pre_earnings) / share_price_pre_earnings if future_value else None\n",
    "            for future_value in pre_earnings_future_values\n",
    "        ]\n",
    "\n",
    "        post_earnings_percent_changes = [\n",
    "            (future_value.value - share_price_post_earnings) / share_price_post_earnings if future_value else None\n",
    "            for future_value in post_earnings_future_values\n",
    "        ]\n",
    "        # Interleave the pre and post earnings percent changes to create a single list of percent changes to serve as labels\n",
    "        return [\n",
    "            pre_earnings_percent_changes[0],\n",
    "            post_earnings_percent_changes[0],\n",
    "            pre_earnings_percent_changes[1],\n",
    "            post_earnings_percent_changes[1],\n",
    "            pre_earnings_percent_changes[2],\n",
    "            post_earnings_percent_changes[2]\n",
    "        ]\n",
    "\n",
    "    def _get_company_summary(self):\n",
    "        # \n",
    "        return f\"The following return data was for a company in the \" \\\n",
    "               f\"{self.company.sector} sector in {self.company.location} \" \\\n",
    "               f\"with a market cap of ${self.company.shares * self.historical_trends['SharePricePostFiling'].current.value} \" \\\n",
    "               f\"at the time of the most recent quarterly report.\"\n",
    "    \n",
    "    def _get_historical_trends(self):\n",
    "        # TODO: Verify that the historical trends is being created correctly\n",
    "        intro = f\"The following csv shows the historical trends for the company's \" \\\n",
    "               f\"financial metrics up until the current point in time, \" \\\n",
    "               f\"including both information about the company \" \\\n",
    "               f\"and macroeconomic metrics: \\n\\n\"\n",
    "        # Create a dictionary to store trend values for each historical trend\n",
    "        trend_data = {}\n",
    "        \n",
    "        for key, trend in self.historical_trends.items():\n",
    "            if key not in EXCLUDE_KEYS:\n",
    "                trend_data[key] = {\n",
    "                    'current': trend.current,\n",
    "                    'last_quarter': trend.last_quarter,\n",
    "                    'six_months_ago': trend.six_months_ago,\n",
    "                    'nine_months_ago': trend.nine_months_ago,\n",
    "                    'one_year_ago': trend.one_year_ago,\n",
    "                    'two_years_ago': trend.two_years_ago\n",
    "                }\n",
    "        \n",
    "        # Convert the dictionary to a DataFrame\n",
    "        df = pd.DataFrame.from_dict(trend_data, orient='index')\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index': 'metric'}, inplace=True)\n",
    "\n",
    "        return intro + f\"```csv\\n{df.to_csv(index=False, date_format='%Y-%m-%d')}\\n```\\n\\n\"\n",
    "    \n",
    "    def _get_most_recent_10k_extracts(self) -> Form10KExtracts:\n",
    "        with open(self.most_recent_10k_file) as f:\n",
    "            text = f.read()\n",
    "            file_md5 = hashlib.md5(text.encode()).hexdigest()\n",
    "            with shelve.open('10k_extracts') as tenKExtracts:\n",
    "                if file_md5 in tenKExtracts:\n",
    "                    extracts = tenKExtracts[file_md5]\n",
    "                else:\n",
    "                    extracts = parse_10_k(text, file_md5)\n",
    "                    tenKExtracts[file_md5] = extracts\n",
    "        return extracts\n",
    "\n",
    "\n",
    "    def _get_most_recent_10k_summary(self):\n",
    "        extracts = self._get_most_recent_10k_extracts()\n",
    "        with shelve.open('10k_summaries') as summaries:\n",
    "            if extracts.hash_digest in summaries:\n",
    "                return summaries[extracts.hash_digest]\n",
    "            summary = summarize_10k_extracts(extracts)\n",
    "            summaries[extracts.hash_digest] = summary\n",
    "            return summary\n",
    "\n",
    "\n",
    "    \n",
    "    def to_anonymous_report(self):\n",
    "        \"\"\"\n",
    "        Details that determine which company is being reported on are redacted,\n",
    "        and a report is returned as a markdown file consisting of the following sections:\n",
    "        - Company Summary\n",
    "        - Historical Trends up until the current point in time including both information about the company and macroeconomic metrics, as a csv file\n",
    "        - Key excerpts from the most recent 10-K report, including when the report was filed\n",
    "        \"\"\"\n",
    "        print(f\"Getting summary for {self.company.name}, Q{self.q}, {self.year}\")\n",
    "        company_summary = self._get_company_summary()\n",
    "        print(f\"Getting historical trends for {self.company.name}, Q{self.q}, {self.year}\")\n",
    "        historical_trends = self._get_historical_trends()\n",
    "        print(f\"Getting most recent 10-K extracts for {self.company.name}, Q{self.q}, {self.year}\")\n",
    "        file_10k_extracts_summary = self._get_most_recent_10k_summary()\n",
    "        \n",
    "        return f\"\"\"\n",
    "# Company Summary\n",
    "{company_summary}\n",
    "\n",
    "# Historical Trends\n",
    "{historical_trends}\n",
    "\n",
    "#  Most Recent 10-K Summary\n",
    "{file_10k_extracts_summary}\n",
    "\"\"\"\n",
    "        \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HistoricalDataStore:\n",
    "    \"\"\"A data structure for storing and querying historical financial data by year and quarter.\"\"\"\n",
    "    _data: Dict[int, Dict[int, Dict[str, PointInTimeValue]]]  # year -> quarter -> metrics\n",
    "    _keys: Dict[str, str] = field(default_factory=Dict[str, str])\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._data = defaultdict(lambda: defaultdict(dict))\n",
    "        self._keys = dict()\n",
    "\n",
    "    def add_metrics(self, year: int, quarter: int, metrics: Dict[str, any]) -> None:\n",
    "        \"\"\"Add or update multiple metrics for a specific year and quarter.\"\"\"\n",
    "        self._data[year][quarter].update(metrics)\n",
    "        # Add the keys to the keys dict with the unit of the first value as the value\n",
    "        for k, v in metrics.items():\n",
    "            self._keys[k] = v.unit if isinstance(v, PointInTimeValue) else None\n",
    "    def add_metric(self, year: int, quarter: int, key: str, value: Any) -> None:\n",
    "        \"\"\"Add or update a single metric for a specific year and quarter.\"\"\"\n",
    "        self._data[year][quarter][key] = value\n",
    "        self._keys[key] = value.unit if isinstance(value, PointInTimeValue) else None\n",
    "\n",
    "\n",
    "    def get_by_year_quarter(self, year: int, quarter: int) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get all metrics for a specific year and quarter.\"\"\"\n",
    "        return self._data.get(year, {}).get(quarter)\n",
    "\n",
    "    def get_by_year(self, year: int) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"Get all quarters' data for a specific year.\"\"\"\n",
    "        return dict(self._data.get(year, {}))\n",
    "\n",
    "    def get_by_year_quarter_metric(self, year: int, quarter: int, metric: str) -> Optional[PointInTimeValue]:\n",
    "        return self._data.get(year, {}).get(quarter, {}).get(metric)\n",
    "\n",
    "    def get_by_quarter(self, quarter: int) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"Get data for a specific quarter across all years.\"\"\"\n",
    "        return {year: quarters[quarter] \n",
    "                for year, quarters in self._data.items() \n",
    "                if quarter in quarters}\n",
    "\n",
    "    def get_all_years(self) -> List[int]:\n",
    "        \"\"\"Get all available years.\"\"\"\n",
    "        return sorted(self._data.keys())\n",
    "\n",
    "    def get_all_quarters(self) -> List[int]:\n",
    "        \"\"\"Get all available quarters across all years.\"\"\"\n",
    "        return sorted(set(q for quarters in self._data.values() for q in quarters.keys()))\n",
    "    \n",
    "    def get_all_metrics(self) -> Set[str]:\n",
    "        return self._keys\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        # Lazily iterate over all metrics and convert to dataframe yield dataframe rows for each quarter, year, using the q, year, and sorted keys within metric as header\n",
    "        keys = sorted(self._keys.items(), key=lambda x: x[0])\n",
    "        # set up dataframe with columns for q, year, and all keys\n",
    "        df = pd.DataFrame(columns=['q', 'year'] + keys)\n",
    "        for year in self.get_all_years():\n",
    "            for q in self.get_by_year(year).values():\n",
    "    \n",
    "                for metrics in q.values():\n",
    "                    df = df.append({'q': q, 'year': year, **metrics}, ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def generate_csv(self) -> Generator[str, None, None]:\n",
    "        # lazily iterate over all metrics and yield csv rows for each quarter, year, using the q, year, and sorted keys within metric as header\n",
    "        # emit header row first with all keys\n",
    "        keys = sorted(self._keys.items(), key=lambda x: x[0])\n",
    "        keys_with_units = [f\"{k} ({v})\" for k, v in keys]\n",
    "        yield ','.join(['q', 'year'] + keys_with_units) + '\\n'\n",
    "        for year in self.get_all_years():\n",
    "            for q in self.get_by_year(year).keys():\n",
    "                quarter_year = [str(q), str(year)]\n",
    "                values = [str(self.get_by_year_quarter_metric(year, q, k).value) if self.get_by_year_quarter_metric(year, q, k) else '' for k, _ in keys]\n",
    "                row = ','.join(quarter_year + values) + '\\n'\n",
    "                yield row\n",
    "    \n",
    "@dataclass\n",
    "class KeyCompanyFacts:\n",
    "    company: Company\n",
    "    historical_data: HistoricalDataStore\n",
    "    submissions_df: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    out_folder = f'out/'\n",
    "    submissions_folder = f'findata/submissions'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.download_folder = f'out/{self.company.cik}/downloads'\n",
    "        \n",
    "    def __str__(self):\n",
    "        # Assemble csv of all metrics and their values\n",
    "        return ''.join(self.historical_data.generate_csv())\n",
    "        \n",
    "    def years(self):\n",
    "        # return all years in the historical data store\n",
    "        return self.historical_data.get_all_years()\n",
    "    \n",
    "    def get_trends(self, q: int, year: int) -> Dict[str, Trend]:\n",
    "        \"\"\"Get trends for all metrics at a specific quarter and year.\"\"\"\n",
    "        current_data = self.historical_data.get_by_year_quarter(year, q) or {}\n",
    "        return {\n",
    "            key: self.get_trend(key, q, year)\n",
    "            for key in current_data.keys()\n",
    "        }\n",
    "\n",
    "    def get_trend(self, key: str, q: int, year: int) -> Trend:\n",
    "        \"\"\"Get trend data for a specific metric.\"\"\"\n",
    "        def get_value(y: int, qtr: int) -> Optional[Any]:\n",
    "            data = self.historical_data.get_by_year_quarter(y, qtr)\n",
    "            return data.get(key) if data else None\n",
    "\n",
    "        return Trend(\n",
    "            current=get_value(year, q),\n",
    "            two_years_ago=get_value(year - 2, q),\n",
    "            one_year_ago=get_value(year - 1, q),\n",
    "            nine_months_ago=get_value(\n",
    "                year if q == 4 else year - 1,\n",
    "                (q % 4) + 1\n",
    "            ),\n",
    "            six_months_ago=get_value(\n",
    "                year if q > 2 else year - 1,\n",
    "                (q - 2) if q > 2 else (q + 2) % 4\n",
    "            ),\n",
    "            last_quarter=get_value(\n",
    "                year if q > 1 else year - 1,\n",
    "                4 if q == 1 else q - 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _load_submissions_df(self) -> pd.DataFrame:\n",
    "        submissions = f'{getcwd()}/{self.submissions_folder}'\n",
    "        os.makedirs(submissions, exist_ok=True)\n",
    "        print(f\"submissions: {submissions}\")\n",
    "        with open(f'{submissions}/CIK{self.company.cik}.json') as f:\n",
    "            obj = json.load(f)\n",
    "            dataframe = pd.DataFrame(obj['filings']['recent'])\n",
    "            for file in obj['filings']['files']:\n",
    "                with open(f\"{submissions}/{file['name']}\") as s:\n",
    "                    file_dataframe = pd.DataFrame(json.load(s))\n",
    "                    dataframe = pd.concat([dataframe, file_dataframe])\n",
    "        self.submissions_df = dataframe\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "    def _load_quarterly_report_index(self) -> pd.DataFrame:\n",
    "        if self.submissions_df.empty:\n",
    "            self._load_submissions_df()\n",
    "        submissions = self.submissions_df\n",
    "        quarterly_reports = submissions[submissions[\"form\"].isin([\"10-K\", \"10-Q\", \"10-Q/A\", \"10-K/A\"])].sort_values(by=\"reportDate\")\n",
    "        if len(quarterly_reports) > 0:\n",
    "            quarterly_reports = quarterly_reports[quarterly_reports['primaryDocument'] != '']\n",
    "            quarterly_reports.dropna(subset=['primaryDocument'], inplace=True)\n",
    "            quarterly_reports['url'] = quarterly_reports.apply(lambda row: create_report_url(self.company.cik, row), axis=1)\n",
    "            quarterly_reports['filingDate'] = pd.to_datetime(quarterly_reports['filingDate'])\n",
    "            quarterly_reports['year'] = quarterly_reports['filingDate'].dt.year\n",
    "            quarterly_reports['q'] = quarterly_reports['filingDate'].dt.quarter\n",
    "        return quarterly_reports\n",
    "\n",
    "\n",
    "    def _download_quarterly_report(self, year: int, q: int):\n",
    "        index = self._load_quarterly_report_index()\n",
    "        url = index.loc[(index['year'] == year) & (index['q'] == q), 'url'].iloc[0]\n",
    "        print(url)\n",
    "        q_folder = f'{self.download_folder}/{year}/{q}'\n",
    "        os.makedirs(q_folder, exist_ok=True)\n",
    "        try:\n",
    "            path = f'{q_folder}/raw.htm'\n",
    "\n",
    "            print(f\"Downloading file to {path}\")\n",
    "            response = requests.get(url, stream=True, headers={\n",
    "                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "                'accept-language': 'en-US,en;q=0.9,he-IL;q=0.8,he;q=0.7',\n",
    "                'cache-control': 'max-age=0',\n",
    "                'priority': 'u=0, i',\n",
    "                'sec-ch-ua': '\"Chromium\";v=\"130\", \"Google Chrome\";v=\"130\", \"Not?A_Brand\";v=\"99\"',\n",
    "                'sec-ch-ua-mobile': '?0',\n",
    "                'sec-ch-ua-platform': '\"macOS\"',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'upgrade-insecure-requests': '1',\n",
    "                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',\n",
    "            })\n",
    "            response.raise_for_status()\n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "            return path\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "    def _download_quarterly_reports(self):\n",
    "        # Iterate over historical data store and find file urls in index for corresponding quarterly reports.\n",
    "        for year in self.historical_data.get_all_years():\n",
    "            for q, _ in self.historical_data.get_by_year(year).items():\n",
    "                self._download_quarterly_report(year, q)\n",
    "\n",
    "    def get_quarterly_report_file(self, year: int, q: int) -> str:\n",
    "        path = f'{self.download_folder}/{year}/{q}/raw.htm'\n",
    "        if not os.path.exists(path):\n",
    "            self._download_quarterly_report(year, q)\n",
    "        return path\n",
    "    \n",
    "    def get_most_recent_10k_file(self, as_of_year: int, as_of_q: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Gets the most recent 10-K file *as of* a given year and quarter.\n",
    "\n",
    "        Args:\n",
    "            as_of_year: The year to consider as the cutoff.\n",
    "            as_of_q: The quarter to consider as the cutoff.\n",
    "\n",
    "        Returns:\n",
    "            The path to the downloaded 10-K file, or None if not found.\n",
    "        \"\"\"\n",
    "        index = self._load_quarterly_report_index()\n",
    "        # Filter for 10-K forms\n",
    "        ten_ks = index[index['form'] == '10-K']\n",
    "        # Filter for reports with year and quarter LESS THAN OR EQUAL TO the as_of values\n",
    "        ten_ks_before_cutoff = ten_ks[(ten_ks['year'] < as_of_year) | ((ten_ks['year'] == as_of_year) & (ten_ks['q'] <= as_of_q))]\n",
    "\n",
    "        if not ten_ks_before_cutoff.empty:\n",
    "            most_recent_10k = ten_ks_before_cutoff.sort_values(by=['year', 'q'], ascending=False).iloc[0]\n",
    "            return self._download_quarterly_report(most_recent_10k['year'], most_recent_10k['q'])\n",
    "\n",
    "        return None\n",
    "        \n",
    "    def add_company_facts(self, facts: Dict[str, Any]) -> None:\n",
    "        \"\"\"Add company facts to the historical data store.\"\"\"\n",
    "        quarterly_report_index = self._load_quarterly_report_index()\n",
    "\n",
    "        for key, unit_data in facts.items():\n",
    "            if unit_data:\n",
    "                for unit, points_in_time in unit_data.items():\n",
    "                    for val in points_in_time:\n",
    "                        if val.get('form') not in ['10-K', '10-Q', '10-Q/A', '10-K/A']:\n",
    "                            continue\n",
    "                        point = to_point_in_time(unit, val)\n",
    "                        if not point:\n",
    "                            print(f\"Could not create point for {key} with unit {unit} and value {val}\")\n",
    "                            continue\n",
    "                    \n",
    "                        # Check if we need to add share prices for a new time period\n",
    "                        existing_data = self.historical_data.get_by_year_quarter(point.year, point.q)\n",
    "                        if not existing_data:\n",
    "                            share_price_pre_filing, share_price_post_filing = get_report_share_prices(self.company, val)\n",
    "                            accn = val[\"accn\"]\n",
    "                            quarterly_report_row = quarterly_report_index[quarterly_report_index[\"accessionNumber\"] == accn]\n",
    "                            accn = accn.replace(\"-\", \"\")\n",
    "                            primary_document = quarterly_report_row[\"primaryDocument\"].values[0] if len(quarterly_report_row) > 0 else None\n",
    "                            quarterly_report_url = f\"https://www.sec.gov/Archives/edgar/data/{self.company.cik}/{accn}/{primary_document}\"\n",
    "                            macroeconomic_metrics = get_macro_metrics(point.year, point.q)\n",
    "                            self.historical_data.add_metrics(point.year, point.q, {\n",
    "                                'SharePricePreFiling': share_price_pre_filing,\n",
    "                                'SharePricePostFiling': share_price_post_filing,\n",
    "                                'QuarterlyReportAccessionNumber': PointInTimeValue(q=point.q, year=point.year, value=accn, unit=''),\n",
    "                                'QuarterlyReportUrl': PointInTimeValue(q=point.q, year=point.year, value=quarterly_report_url, unit='URL') if quarterly_report_url else None,\n",
    "                                **macroeconomic_metrics\n",
    "                            })\n",
    "                        \n",
    "                        # Add the new metric\n",
    "                        self.historical_data.add_metric(point.year, point.q, key, point)\n",
    "\n",
    "    def get_projection(self, q: int, year: int) -> Projection:\n",
    "        \"\"\"\n",
    "        A projection in this case is not a guess about what will happen, but because we are looking at historical data\n",
    "        we know about what happened in the years and quarters after the q and year we are querying.\n",
    "\n",
    "        It contains the values of the stock price pre-earnings and post-earnings for the next quarter, six months, and year following the provided quarter and year.\n",
    "        \"\"\"\n",
    "        # get pre-earnings and post-earnings for next quarter, six months, and year:\n",
    "        next_quarter_pre_earnings = self.historical_data.get_by_year_quarter_metric(year, q + 1 if q < 4 else 1, 'SharePricePreFiling')\n",
    "        next_quarter_post_earnings = self.historical_data.get_by_year_quarter_metric(year, q + 1 if q < 4 else 1, 'SharePricePostFiling')\n",
    "        next_six_months_pre_earnings = self.historical_data.get_by_year_quarter_metric(year, q + 2 if q < 3 else q - 2, 'SharePricePreFiling')\n",
    "        next_six_months_post_earnings = self.historical_data.get_by_year_quarter_metric(year, q + 2 if q < 3 else q - 2, 'SharePricePostFiling')\n",
    "        next_year_pre_earnings = self.historical_data.get_by_year_quarter_metric(year + 1, q, 'SharePricePreFiling')\n",
    "        next_year_post_earnings = self.historical_data.get_by_year_quarter_metric(year + 1, q, 'SharePricePostFiling')\n",
    "\n",
    "        return Projection(\n",
    "            next_quarter=Label(\n",
    "                stock_price_pre_earnings=next_quarter_pre_earnings,\n",
    "                stock_price_post_earnings=next_quarter_post_earnings\n",
    "            ),\n",
    "            next_six_months=Label(\n",
    "                stock_price_pre_earnings=next_six_months_pre_earnings,\n",
    "                stock_price_post_earnings=next_six_months_post_earnings\n",
    "            ),\n",
    "            next_year=Label(\n",
    "                stock_price_pre_earnings=next_year_pre_earnings,\n",
    "                stock_price_post_earnings=next_year_post_earnings\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def create_contextual_snapshot(self, year: int, q: int) -> ContextualSnapshot:\n",
    "        return ContextualSnapshot(\n",
    "            year=year,\n",
    "            q=q,\n",
    "            company=self.company,\n",
    "            historical_trends=self.get_trends(q, year),\n",
    "            future_projection=self.get_projection(q, year),\n",
    "            most_recent_10k_file=self.get_most_recent_10k_file(year, q)\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_company_facts_file(cls, company, prompt_cache):\n",
    "        company_facts = \"findata/companyfacts\"\n",
    "        cik = company['cik']\n",
    "        print(f\"cik: {cik}\")\n",
    "        with open(f'{company_facts}/CIK{cik}.json') as f:\n",
    "            print('loading facts')\n",
    "            all_facts = json.load(f)\n",
    "            us_gaap_keys = all_facts['facts']['us-gaap'].keys()\n",
    "            most_relevant_gaap_keys = most_relevant_keys(us_gaap_keys, 20, prompt_cache=prompt_cache)\n",
    "            print(f\"Most relevant gaap keys: {most_relevant_gaap_keys}\")\n",
    "            company_obj = Company(\n",
    "                cik=cik,\n",
    "                ticker=company['Ticker'],\n",
    "                exchange=company['Exchange'],\n",
    "                name=company['Name'],\n",
    "                shares=int(float(company['Quantity'].replace(',', ''))),\n",
    "                sector=company['Sector'],\n",
    "                location=company['Location']\n",
    "            )            \n",
    "            # Create instance with empty historical data\n",
    "            instance = cls(company=company_obj, historical_data=HistoricalDataStore())\n",
    "            # Filter facts to only include most relevant keys\n",
    "            relevant_facts = {\n",
    "                key: all_facts['facts']['us-gaap'][key]['units'] if key in all_facts['facts']['us-gaap'] else None\n",
    "                for key in most_relevant_gaap_keys\n",
    "            }\n",
    "            \n",
    "            # Use add_company_facts to process the data\n",
    "            instance.add_company_facts(relevant_facts)\n",
    "            \n",
    "            return instance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import hashlib\n",
    "\n",
    "@dataclass(frozen=True, eq=True)\n",
    "class PromptResponse:\n",
    "    prompt: str\n",
    "    response: List[str]\n",
    "\n",
    "def hash(long_string):\n",
    "    hashlib.md5(bytes(long_string, 'utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def download_and_inflate(url, output_dir):\n",
    "    \"\"\"Downloads and inflates a zip file to a subdirectory within the output directory.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file to download.\n",
    "        output_dir (str): The main output directory.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(url)\n",
    "    subdirectory = filename.replace(\".zip\", \"\")  # Create subdirectory name\n",
    "    subdirectory_path = os.path.join(output_dir, subdirectory)\n",
    "    zip_filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    if not os.path.exists(subdirectory_path):\n",
    "        print(f\"Downloading file: {filename}\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, headers={\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'accept-language': 'en-US,en;q=0.9,he-IL;q=0.8,he;q=0.7',\n",
    "            'cache-control': 'max-age=0',\n",
    "            'priority': 'u=0, i',\n",
    "            'sec-ch-ua': '\"Chromium\";v=\"130\", \"Google Chrome\";v=\"130\", \"Not?A_Brand\";v=\"99\"',\n",
    "            'sec-ch-ua-mobile': '?0',\n",
    "            'sec-ch-ua-platform': '\"macOS\"',\n",
    "            'sec-fetch-dest': 'document',\n",
    "            'sec-fetch-mode': 'navigate',\n",
    "            'sec-fetch-site': 'none',\n",
    "            'sec-fetch-user': '?1',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',\n",
    "        })\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            with open(zip_filepath, \"wb\") as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Download complete: {filename}\")\n",
    "\n",
    "            # Create the subdirectory if it doesn't exist\n",
    "            if not os.path.exists(subdirectory_path):\n",
    "                os.makedirs(subdirectory_path)\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(subdirectory_path)\n",
    "            print(f\"File inflated to: {subdirectory_path}\")\n",
    "\n",
    "            # Optionally remove the zip file after extraction\n",
    "            os.remove(zip_filepath)\n",
    "            print(f\"Removed zip file: {zip_filepath}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading file: {e}\")\n",
    "        except zipfile.BadZipFile as e:\n",
    "            print(f\"Error inflating file: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "    else:\n",
    "        print(f\"Zip File already exists: {zip_filepath}\")\n",
    "        subdirectory = filename.replace(\".zip\", \"\")  # Create subdirectory name\n",
    "        subdirectory_path = os.path.join(output_dir, subdirectory)\n",
    "        if os.path.exists(subdirectory_path):\n",
    "            print(f\"Subdirectory already exists: {subdirectory_path}\")\n",
    "        else:\n",
    "            print(\"Zip file exists but subdirectory does not. Please manually extract the zip file.\")\n",
    "\n",
    "# Example usage\n",
    "urls = [\n",
    "    \"http://www.sec.gov/Archives/edgar/daily-index/xbrl/companyfacts.zip\",\n",
    "    \"https://www.sec.gov/Archives/edgar/daily-index/bulkdata/submissions.zip\"\n",
    "]\n",
    "output_dir = f\"{getcwd()}/findata\"\n",
    "\n",
    "for url in urls:\n",
    "    download_and_inflate(url, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "facts = KeyCompanyFacts.from_company_facts_file(company=holdings.iloc[0], prompt_cache={})\n",
    "snapshot = facts.create_contextual_snapshot(2020, 1)\n",
    "extracts = snapshot._get_most_recent_10k_extracts()\n",
    "\n",
    "print(len(extracts.md_and_a), len(extracts.disclosures), len(extracts.risk_factors))\n",
    "print(snapshot.to_anonymous_report())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shelve\n",
    "import json\n",
    "\n",
    "def create_resilient_training_data(holdings, outdir, processed_ciks_shelf='processed_ciks', \n",
    "                                 prompt_cache_shelf='prompt_cache', \n",
    "                                 training_data_file='training_data.parquet'):\n",
    "    \"\"\"\n",
    "    Create a resilient training data DataFrame with checkpointing at every snapshot.\n",
    "    \n",
    "    Args:\n",
    "        holdings (pd.DataFrame): DataFrame containing company holdings\n",
    "        outdir (str): Output directory for snapshots\n",
    "        processed_ciks_shelf (str): Shelf file for tracking processed CIKs\n",
    "        prompt_cache_shelf (str): Shelf file for prompt caching\n",
    "        training_data_file (str): Parquet file to save/load training data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Completed training data DataFrame\n",
    "    \"\"\"\n",
    "    # Attempt to load existing training data\n",
    "    if os.path.exists(training_data_file):\n",
    "        training_data = pd.read_parquet(training_data_file)\n",
    "        # Get the last processed snapshots to continue from\n",
    "        processed_snapshots = set(\n",
    "            training_data.apply(\n",
    "                lambda x: f\"{x['cik']}_{x['year']}_{x['q']}\", \n",
    "                axis=1\n",
    "            )\n",
    "        ) if not training_data.empty else set()\n",
    "    else:\n",
    "        training_data = pd.DataFrame()\n",
    "        processed_snapshots = set()\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Open shelves for processed CIKs and prompt cache\n",
    "    with shelve.open(processed_ciks_shelf) as processed, \\\n",
    "         shelve.open(prompt_cache_shelf) as prompt_cache:\n",
    "        \n",
    "        # Iterate through holdings\n",
    "        for i, company in holdings.iterrows():\n",
    "            try:\n",
    "                # Process company facts\n",
    "                company_facts = KeyCompanyFacts.from_company_facts_file(\n",
    "                    company=holdings.iloc[i], \n",
    "                    prompt_cache=prompt_cache\n",
    "                )\n",
    "                \n",
    "                # Create company-specific directory\n",
    "                company_dir = os.path.join(os.getcwd(), outdir, str(company_facts.company.cik))\n",
    "                print(f\"Company facts exist for {company_facts.years()}\")\n",
    "                \n",
    "                for year in company_facts.years():\n",
    "                    for q in range(1, 5):\n",
    "                        # Create snapshot identifier\n",
    "                        snapshot_id = f\"{company_facts.company.cik}_{year}_{q}\"\n",
    "                        \n",
    "                        # # Skip if this snapshot was already processed\n",
    "                        # if snapshot_id in processed_snapshots:\n",
    "                        #     continue\n",
    "                            \n",
    "                        try:\n",
    "                            # Check if data is available for this year and quarter\n",
    "                            if company_facts.historical_data.get_by_year_quarter(year, q):\n",
    "                                # Create contextual snapshot\n",
    "                                snapshot = company_facts.create_contextual_snapshot(year, q)\n",
    "                                \n",
    "                                # Prepare file paths\n",
    "                                snapshot_dir = os.path.join(company_dir, 'snapshots', str(year))\n",
    "                                os.makedirs(snapshot_dir, exist_ok=True)\n",
    "                                snapshot_file = os.path.join(snapshot_dir, f'{q}.md')\n",
    "                                labels_file = os.path.join(snapshot_dir, f'{q}-labels.json')\n",
    "                                \n",
    "                                # Save snapshot\n",
    "                                with open(snapshot_file, 'w') as f:\n",
    "                                    f.write(snapshot.to_anonymous_report())\n",
    "\n",
    "                                # Get and save labels\n",
    "                                labels = {\n",
    "                                    'cik': company_facts.company.cik,\n",
    "                                    'year': year,\n",
    "                                    'q': q,\n",
    "                                    'snapshot_file': snapshot_file,\n",
    "                                    'labels': snapshot.get_labels()\n",
    "                                }\n",
    "                                \n",
    "                                # Save labels to JSON\n",
    "                                with open(labels_file, 'w') as f:\n",
    "                                    json.dump(labels, f)\n",
    "\n",
    "                                # Add single row to training data\n",
    "                                training_data = pd.concat(\n",
    "                                    [training_data, pd.DataFrame([labels])], \n",
    "                                    ignore_index=True\n",
    "                                )\n",
    "                                \n",
    "                                # Save progress after each snapshot\n",
    "                                training_data.to_parquet(training_data_file, index=False)\n",
    "                                \n",
    "                                # Mark snapshot as processed\n",
    "                                processed_snapshots.add(snapshot_id)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing snapshot {snapshot_id}: {e}\")\n",
    "                            traceback.print_exc()\n",
    "                            continue\n",
    "\n",
    "                # Mark CIK as fully processed only if all snapshots succeeded\n",
    "                if all(\n",
    "                    f\"{company_facts.company.cik}_{year}_{q}\" in processed_snapshots\n",
    "                    for year in company_facts.years()\n",
    "                    for q in range(1, 5)\n",
    "                    if company_facts.historical_data.get_by_year_quarter(year, q)\n",
    "                ):\n",
    "                    processed[str(company_facts.company.cik)] = True\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing company {company['cik']}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    return training_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "training_data = create_resilient_training_data(holdings, outdir)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
